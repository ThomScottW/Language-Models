{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.txt', 'r') as f:\n",
    "  text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the characters in the dataset: 5458199\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the characters in the dataset: {len(text)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary\n",
    "The vocabulary is the set of models that the model can see or emit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"#%&'()*,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[]_`abcdefghijklmnopqrstuvwxyz|}~\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding (Tokenizing)\n",
    "Here we define a simple character-level encoder, and use it to encode the first\n",
    "few hundred characters of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# These two just map the characters to their position in the vocabulary\n",
    "_ENCODER_MAPPING = {\n",
    "    character: position for position, character in enumerate(chars)\n",
    "}\n",
    "\n",
    "_DECODER_MAPPING = {\n",
    "  position: character for position, character in enumerate(chars)\n",
    "}\n",
    "\n",
    "def encode(input_text: str) -> List[int]:\n",
    "  \"\"\"Encode a string of text to an integer vector.\"\"\"\n",
    "  return [_ENCODER_MAPPING[character] for character in input_text]\n",
    "\n",
    "def decode(input_vector: List[int]) -> str:\n",
    "  \"\"\"Decode an integer vector into a string of text.\"\"\"\n",
    "  return ''.join([_DECODER_MAPPING[value] for value in input_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5458199]) torch.int64\n",
      "tensor([51, 69, 70, 80,  1, 70, 80,  1, 81, 69, 66,  1, 16, 15, 15, 81, 69,  1,\n",
      "        36, 81, 66, 85, 81,  1, 67, 70, 73, 66,  1, 77, 79, 66, 80, 66, 75, 81,\n",
      "        66, 65,  1, 63, 86,  1, 47, 79, 76, 71, 66, 64, 81,  1, 38, 82, 81, 66,\n",
      "        75, 63, 66, 79, 68, 11,  1, 62, 75, 65,  0, 70, 80,  1, 77, 79, 66, 80,\n",
      "        66, 75, 81, 66, 65,  1, 70, 75,  1, 64, 76, 76, 77, 66, 79, 62, 81, 70,\n",
      "        76, 75,  1, 84, 70, 81, 69,  1, 54, 76])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_TRAINING_SPLIT_FRACTION = 0.9\n",
    "_TRAINING_SPLIT_INDEX = int(_TRAINING_SPLIT_FRACTION * len(data))\n",
    "training_data = data[:_TRAINING_SPLIT_INDEX]\n",
    "validation_data = data[_TRAINING_SPLIT_INDEX:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blocks\n",
    "The block size is the maximum size of the text that can be fed into the transformer at once. When training a transformer, the entire training text isn't shoved into the transformer at once, doing so would be computationally prohibitive.\n",
    "\n",
    "When a block of data is sampled like in the first cell below, it's actually multiple examples packed into one due to the fact that the tokens follow one another in the actual text.\n",
    "\n",
    "Additionally, selecting a certain block size gets the transformer used to seeing context lengths of size 1 all the way to the block size. This means that the block size determines the maximum context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([51, 69, 70, 80,  1, 70, 80,  1, 81])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "training_data[:8 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([51]), the target is: 69\n",
      "When the input is tensor([51, 69]), the target is: 70\n",
      "When the input is tensor([51, 69, 70]), the target is: 80\n",
      "When the input is tensor([51, 69, 70, 80]), the target is: 1\n",
      "When the input is tensor([51, 69, 70, 80,  1]), the target is: 70\n",
      "When the input is tensor([51, 69, 70, 80,  1, 70]), the target is: 80\n",
      "When the input is tensor([51, 69, 70, 80,  1, 70, 80]), the target is: 1\n",
      "When the input is tensor([51, 69, 70, 80,  1, 70, 80,  1]), the target is: 81\n",
      "These are the 8 examples hidden in a chunk of text of length 9\n"
     ]
    }
   ],
   "source": [
    "transformer_inputs = training_data[:block_size]\n",
    "transformer_targets = training_data[1:block_size + 1]  # Targets for each position in the input\n",
    "for t in range(block_size):\n",
    "  context = transformer_inputs[:t + 1]\n",
    "  target = transformer_targets[t]\n",
    "  print(f'When the input is {context}, the target is: {target}')\n",
    "print(f'These are the {block_size} examples hidden in a chunk of text of length {block_size + 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the batch dimension for parallel computing on GPUs\n",
    "\n",
    "torch.randint docs: https://pytorch.org/docs/stable/generated/torch.randint.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor size: torch.Size([4, 8])\n",
      "tensor([[65,  1, 64, 62, 82, 73, 65, 79],\n",
      "        [67, 62, 64, 66, 26,  0,  1,  1],\n",
      "        [ 1, 70, 80,  1, 81, 69, 66,  1],\n",
      "        [13,  1, 43, 66, 81,  7, 80,  1]])\n",
      "Target tensor size: torch.Size([4, 8])\n",
      "tensor([[ 1, 64, 62, 82, 73, 65, 79, 76],\n",
      "        [62, 64, 66, 26,  0,  1,  1,  1],\n",
      "        [70, 80,  1, 81, 69, 66,  1, 67],\n",
      "        [ 1, 43, 66, 81,  7, 80,  1, 79]])\n",
      "When the input is [65], the target is: 1\n",
      "When the input is [65, 1], the target is: 64\n",
      "When the input is [65, 1, 64], the target is: 62\n",
      "When the input is [65, 1, 64, 62], the target is: 82\n",
      "When the input is [65, 1, 64, 62, 82], the target is: 73\n",
      "When the input is [65, 1, 64, 62, 82, 73], the target is: 65\n",
      "When the input is [65, 1, 64, 62, 82, 73, 65], the target is: 79\n",
      "When the input is [65, 1, 64, 62, 82, 73, 65, 79], the target is: 76\n",
      "When the input is [67], the target is: 62\n",
      "When the input is [67, 62], the target is: 64\n",
      "When the input is [67, 62, 64], the target is: 66\n",
      "When the input is [67, 62, 64, 66], the target is: 26\n",
      "When the input is [67, 62, 64, 66, 26], the target is: 0\n",
      "When the input is [67, 62, 64, 66, 26, 0], the target is: 1\n",
      "When the input is [67, 62, 64, 66, 26, 0, 1], the target is: 1\n",
      "When the input is [67, 62, 64, 66, 26, 0, 1, 1], the target is: 1\n",
      "When the input is [1], the target is: 70\n",
      "When the input is [1, 70], the target is: 80\n",
      "When the input is [1, 70, 80], the target is: 1\n",
      "When the input is [1, 70, 80, 1], the target is: 81\n",
      "When the input is [1, 70, 80, 1, 81], the target is: 69\n",
      "When the input is [1, 70, 80, 1, 81, 69], the target is: 66\n",
      "When the input is [1, 70, 80, 1, 81, 69, 66], the target is: 1\n",
      "When the input is [1, 70, 80, 1, 81, 69, 66, 1], the target is: 67\n",
      "When the input is [13], the target is: 1\n",
      "When the input is [13, 1], the target is: 43\n",
      "When the input is [13, 1, 43], the target is: 66\n",
      "When the input is [13, 1, 43, 66], the target is: 81\n",
      "When the input is [13, 1, 43, 66, 81], the target is: 7\n",
      "When the input is [13, 1, 43, 66, 81, 7], the target is: 80\n",
      "When the input is [13, 1, 43, 66, 81, 7, 80], the target is: 1\n",
      "When the input is [13, 1, 43, 66, 81, 7, 80, 1], the target is: 79\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "torch.manual_seed(1)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "\n",
    "def get_batch(split: str) -> Tuple[torch.tensor, torch.tensor]:\n",
    "  \"\"\"Get a batch of data containing training and target sequences.\n",
    "\n",
    "  Choose batch_size pairs of randomly chosen input and target blocks.\n",
    "  \n",
    "  Args:\n",
    "    split: A string specifying whether the returned batch should be from\n",
    "      the training set or the validation set. \"train\" is checked for, while\n",
    "      anything else will be considered validation.\n",
    "  Returns:\n",
    "    inputs, targets: Two batch_size x block_size tensors, where the i'th row\n",
    "      in each tensor corresponds to the input and prediction targets.\n",
    "  \"\"\"\n",
    "  data = training_data if split == 'train' else validation_data\n",
    "\n",
    "  # Choose batch_size number of random offsets between 0 and the length\n",
    "  # of the data - the block size.\n",
    "  randomly_chosen_indices = torch.randint(len(data) - block_size, (batch_size,))\n",
    "  inputs = torch.stack(\n",
    "      [data[i:i + block_size] for i in randomly_chosen_indices]\n",
    "  )\n",
    "  targets = torch.stack(\n",
    "      [data[i + 1:i + block_size + 1] for i in randomly_chosen_indices]\n",
    "  )\n",
    "  return inputs, targets\n",
    "\n",
    "input_batch, target_batch = get_batch('train')\n",
    "print(f'Input tensor size: {input_batch.shape}')\n",
    "print(input_batch)\n",
    "print(f'Target tensor size: {target_batch.shape}')\n",
    "print(target_batch)\n",
    "\n",
    "for b in range(batch_size):\n",
    "  for t in range(block_size):\n",
    "    context = input_batch[b, :t+1]\n",
    "    target = target_batch[b, t]\n",
    "    print(f'When the input is {context.tolist()}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Simple: Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 91])\n",
      "tensor(4.9227, grad_fn=<NllLossBackward0>)\n",
      "\n",
      ".)k6`lcb(v,}y_<tRwfKd>eRwO@c_LAU:}|z@Y70zb[-ZfD\n",
      "CkcJh[O2]1L0}f@*X]Aq;`OtIO1@6D],5IOL&2L[m*#lA~x-1u\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "  def __init__(self, vocabulary_size):\n",
    "    super().__init__()\n",
    "\n",
    "    # The embedding table is basically just a tensor of vocab_size x vocab_size\n",
    "    self.token_embedding_table = nn.Embedding(vocabulary_size, vocabulary_size)\n",
    "  \n",
    "  def forward(self, index, targets=None):\n",
    "    # index and targets are both (B, T) tensors of integers\n",
    "    # B = batch_size\n",
    "    # T = time = block_size\n",
    "    # C = channels = vocab_size\n",
    "    # logits are the scores for the next token\n",
    "    logits = self.token_embedding_table(index)  # (B,T,C)\n",
    "\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html\n",
    "    # Pytorch's cross_entropy function expects a tensor of B, C, so here\n",
    "    # the logits and targets tensors are reshaped to accomodate this\n",
    "    if targets is None:\n",
    "      loss = None\n",
    "    else:\n",
    "      B, T, C = logits.shape\n",
    "      logits = logits.view(B*T, C)\n",
    "      targets = targets.view(B*T)\n",
    "      loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "    return logits, loss\n",
    "  \n",
    "  def generate(self, index, max_new_tokens):\n",
    "    \"\"\"Generate predictions on the characters given by index.\n",
    "    \n",
    "    Args:\n",
    "      index: Current context of some characters in a batch. Shape BxT. This\n",
    "        function's job is to extend index to be Bx(T+1), Bx(T+2), etc for\n",
    "        max_new_tokens.\n",
    "      max_new_tokens: How many additional tokens to predict from the given\n",
    "        context.\n",
    "    \"\"\"\n",
    "    # index is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "      \n",
    "      # Get the predictions for the current context\n",
    "      logits, loss = self(index)\n",
    "\n",
    "      # Focus only on the last time step\n",
    "      logits = logits[:, -1, :]  # becomes (B, C)\n",
    "\n",
    "      # Apply softmax to get the probabilities\n",
    "      probs = F.softmax(logits, dim=-1)  #(B, C)\n",
    "\n",
    "      # Sample from the distribution\n",
    "      index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "\n",
    "      # Append the sampled index to the running sequence\n",
    "      index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "      \n",
    "    return index\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "bigram_language_model = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram_language_model(input_batch, target_batch)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "# Feeding in a 1x1 tensor with a zero in it is equivalent to passing a newline\n",
    "# as the first token, or whatever the first character in the vocabulary is.\n",
    "index = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# This will be a Bx(T+1) output, and since the prior line of code passed in\n",
    "# 1 batch, B is 1.\n",
    "generations = bigram_language_model.generate(index, max_new_tokens=100)\n",
    "generations = generations[0].tolist()\n",
    "\n",
    "print(decode(generations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigram_language_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.452397346496582\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(30000):\n",
    "  input_data, target_data = get_batch('train')\n",
    "\n",
    "  logits, loss = bigram_language_model(input_data, target_data)\n",
    "  optimizer.zero_grad(set_to_none=True)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " a   hild    geaveseast  RERelllelme  he,\n",
      " awint  shithathodsepelim  w  IRI  pithadil meblal ske   l\n"
     ]
    }
   ],
   "source": [
    "# Feeding in a 1x1 tensor with a zero in it is equivalent to passing a newline\n",
    "# as the first token, or whatever the first character in the vocabulary is.\n",
    "index = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# This will be a Bx(T+1) output, and since the prior line of code passed in\n",
    "# 1 batch, B is 1.\n",
    "generations = bigram_language_model.generate(index, max_new_tokens=100)\n",
    "generations = generations[0].tolist()\n",
    "\n",
    "print(decode(generations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an inefficient case using a for loop to make each token the average of the tokens before it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, T, C = 4, 8, 2\n",
    "data = torch.randn(B, T, C)\n",
    "\n",
    "bag_of_words = torch.zeros((B, T, C))\n",
    "\n",
    "for batch in range(B):\n",
    "\tfor timestep in range(T):\n",
    "\t\tprevious = data[batch, :timestep+1]\n",
    "\t\tbag_of_words[batch, timestep] = torch.mean(previous, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3996, -0.2986],\n",
       "        [-0.1525, -0.5859],\n",
       "        [ 0.4731,  1.4990],\n",
       "        [-1.7875,  0.7657],\n",
       "        [-0.7264, -1.1719],\n",
       "        [ 0.7395, -1.7191],\n",
       "        [-0.2197, -0.3693],\n",
       "        [-0.8298,  0.1927]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3996, -0.2986],\n",
       "        [-0.2760, -0.4422],\n",
       "        [-0.0263,  0.2049],\n",
       "        [-0.4666,  0.3451],\n",
       "        [-0.5186,  0.0417],\n",
       "        [-0.3089, -0.2518],\n",
       "        [-0.2961, -0.2686],\n",
       "        [-0.3629, -0.2109]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using matrix multiplication to speed things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "-----\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "-----\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# This just ends up making each column of the resulting matrix\n",
    "# equal to the sum of each column in b.\n",
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "print(f'a=\\n{a}\\n-----')\n",
    "print(f'b=\\n{b}\\n-----')\n",
    "print(f'c=\\n{c}\\n-----')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
